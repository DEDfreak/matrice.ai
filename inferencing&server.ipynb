{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pbF66LPar3l",
        "outputId": "1c8fe465-3789-43d5-ad27-27200f3260c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['--2025-02-17 05:39:44--  https://drive.google.com/uc?export=download&id=1j71BjISymIyAqDbsuxNjBj4nqx9ozF0e',\n",
              " 'Resolving drive.google.com (drive.google.com)... 142.250.141.100, 142.250.141.113, 142.250.141.139, ...',\n",
              " 'Connecting to drive.google.com (drive.google.com)|142.250.141.100|:443... connected.',\n",
              " 'HTTP request sent, awaiting response... 303 See Other',\n",
              " 'Location: https://drive.usercontent.google.com/download?id=1j71BjISymIyAqDbsuxNjBj4nqx9ozF0e&export=download [following]',\n",
              " '--2025-02-17 05:39:44--  https://drive.usercontent.google.com/download?id=1j71BjISymIyAqDbsuxNjBj4nqx9ozF0e&export=download',\n",
              " 'Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84',\n",
              " 'Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.',\n",
              " 'HTTP request sent, awaiting response... 200 OK',\n",
              " 'Length: 88335 (86K) [application/octet-stream]',\n",
              " 'Saving to: ‘validation_data_cleaned.json’',\n",
              " '',\n",
              " '',\n",
              " '          validatio   0%[                    ]       0  --.-KB/s               ',\n",
              " 'validation_data_cle 100%[===================>]  86.26K  --.-KB/s    in 0.03s   ',\n",
              " '',\n",
              " '2025-02-17 05:39:47 (2.94 MB/s) - ‘validation_data_cleaned.json’ saved [88335/88335]',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1j71BjISymIyAqDbsuxNjBj4nqx9ozF0e' -O validation_data_cleaned.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision eva-decord tqdm numpy mamba-ssm triton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-2F7vxOayxP",
        "outputId": "73996084-1ab6-462d-985d-ddf326372696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Collecting eva-decord\n",
            "  Downloading eva_decord-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (449 bytes)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting mamba-ssm\n",
            "  Downloading mamba_ssm-2.2.4.tar.gz (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Collecting ninja (from mamba-ssm)\n",
            "  Using cached ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (0.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (4.48.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (24.2)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (75.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.5.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (2025.1.31)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Downloading eva_decord-0.6.1-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "Building wheels for collected packages: mamba-ssm\n",
            "  Building wheel for mamba-ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.4-cp311-cp311-linux_x86_64.whl size=323672993 sha256=8a0be01153fa30727a9e69024fbe061eb92c7ba4416d2049c5fc3107ed91d852\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/5e/64/cfcb5dfe4f854944456e031c34953dc872af1ad7c206145d4a\n",
            "Successfully built mamba-ssm\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, eva-decord, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, mamba-ssm\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed eva-decord-0.6.1 mamba-ssm-2.2.4 ninja-1.11.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import decord\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "_V3RzPtMdtTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, ChainedScheduler\n",
        "import decord\n",
        "import numpy as np\n",
        "from mamba_ssm import Mamba\n",
        "from einops import rearrange\n",
        "import torchvision.transforms as T\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "drtwdWswjb1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureAggregatedBiS6(nn.Module):\n",
        "    \"\"\"Enhanced Bi-directional S6 Block\"\"\"\n",
        "    def __init__(self, dim: int, kernel_sizes: List[int] = [3,5,7], expansion: int = 2):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.ConstantPad1d((k//2, (k-1)//2), 0),\n",
        "                nn.Conv1d(dim, dim, k),\n",
        "                nn.GELU()\n",
        "            ) for k in kernel_sizes\n",
        "        ])\n",
        "\n",
        "        self.s6_fwd = Mamba(\n",
        "            d_model=dim,\n",
        "            d_state=16,\n",
        "            d_conv=4,\n",
        "            expand=expansion\n",
        "        )\n",
        "        self.s6_bwd = Mamba(\n",
        "            d_model=dim,\n",
        "            d_state=16,\n",
        "            d_conv=4,\n",
        "            expand=expansion\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.gate = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, C, T = x.shape\n",
        "        residual = x\n",
        "\n",
        "        # Multi-scale temporal aggregation\n",
        "        conv_outs = [conv(x) for conv in self.convs]\n",
        "        x = sum(conv_outs) * self.gate\n",
        "\n",
        "        # Bi-directional processing\n",
        "        x = x.permute(0, 2, 1)  # [B, T, C]\n",
        "        x_fwd = self.s6_fwd(x)\n",
        "        x_bwd = self.s6_bwd(x.flip(1)).flip(1)\n",
        "        x = x_fwd + x_bwd\n",
        "\n",
        "        return self.norm(x.permute(0, 2, 1) + residual)\n",
        "\n",
        "class DualBiS6TAL(nn.Module):\n",
        "    \"\"\"Dual-path S6 Architecture for Temporal Action Localization\"\"\"\n",
        "    def __init__(self, num_classes: int, dim: int = 128, recur_steps: int = 4):\n",
        "        super().__init__()\n",
        "        # Feature extractor\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(3, dim, kernel_size=(3,7,7), stride=(1,2,2), padding=(1,3,3)),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool3d(kernel_size=(1,3,3), stride=(1,2,2), padding=(0,1,1)),\n",
        "            nn.BatchNorm3d(dim)\n",
        "        )\n",
        "\n",
        "        # Temporal modeling\n",
        "        self.temporal_blocks = nn.ModuleList([\n",
        "            FeatureAggregatedBiS6(dim)\n",
        "            for _ in range(recur_steps)\n",
        "        ])\n",
        "\n",
        "        # Pyramid branches\n",
        "        self.pyramid = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                FeatureAggregatedBiS6(dim),\n",
        "                nn.MaxPool1d(2, stride=2)\n",
        "            ) for _ in range(4)\n",
        "        ])\n",
        "\n",
        "        # Prediction heads\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.Conv1d(dim, dim//2, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(dim//2, num_classes, 1)\n",
        "        )\n",
        "\n",
        "        self.reg_head = nn.Sequential(\n",
        "            nn.Conv1d(dim, dim//2, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(dim//2, 2, 1),\n",
        "            nn.Tanh()  # Bounded regression outputs\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict:\n",
        "        # Initial features\n",
        "        x = self.encoder(x)  # [B, C, T, H, W]\n",
        "        x = x.flatten(3).mean(-1)  # [B, C, T]\n",
        "\n",
        "        # Temporal modeling\n",
        "        for block in self.temporal_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Multi-scale pyramid\n",
        "        pyramid_features = [x]\n",
        "        for branch in self.pyramid:\n",
        "            x = branch(x)\n",
        "            pyramid_features.append(x)\n",
        "\n",
        "        # Merge pyramid features\n",
        "        merged = torch.cat([\n",
        "            nn.functional.interpolate(f, size=pyramid_features[0].shape[-1])\n",
        "            for f in pyramid_features\n",
        "        ], dim=1)\n",
        "\n",
        "        return {\n",
        "            'cls_logits': self.cls_head(merged),\n",
        "            'reg_outputs': self.reg_head(merged)\n",
        "        }\n",
        "\n",
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int = 4):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, C, T = x.shape\n",
        "        x = x.permute(0, 2, 1)\n",
        "        qkv = self.qkv(x).reshape(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        x = (attn.softmax(dim=-1) @ v).transpose(1, 2).reshape(B, T, C)\n",
        "        return self.proj(x).permute(0, 2, 1)\n",
        "\n",
        "class RefinedFeatureAggregatedBiS6(nn.Module):\n",
        "    \"\"\"Enhanced Bi-directional S6 Block with temporal attention\"\"\"\n",
        "    def __init__(self, dim: int, kernel_sizes: List[int] = [3,5,7], expansion: int = 2):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.ConstantPad1d((k//2, (k-1)//2), 0),\n",
        "                nn.Conv1d(dim, dim, k),\n",
        "                nn.GELU()\n",
        "            ) for k in kernel_sizes\n",
        "        ])\n",
        "\n",
        "        self.temporal_attn = TemporalAttention(dim)\n",
        "\n",
        "        self.s6_fwd = Mamba(\n",
        "            d_model=dim,\n",
        "            d_state=16,\n",
        "            d_conv=4,\n",
        "            expand=expansion\n",
        "        )\n",
        "        self.s6_bwd = Mamba(\n",
        "            d_model=dim,\n",
        "            d_state=16,\n",
        "            d_conv=4,\n",
        "            expand=expansion\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.gate = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Multi-scale temporal aggregation\n",
        "        conv_outs = [conv(x) for conv in self.convs]\n",
        "        x = sum(conv_outs) * self.gate\n",
        "\n",
        "        # Temporal attention\n",
        "        x = x + self.temporal_attn(x)\n",
        "        x = self.norm1(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        # Bi-directional processing\n",
        "        x_p = x.permute(0, 2, 1)  # [B, T, C]\n",
        "        x_fwd = self.s6_fwd(x_p)\n",
        "        x_bwd = self.s6_bwd(x_p.flip(1)).flip(1)\n",
        "        x = x_fwd + x_bwd\n",
        "        x = self.norm2(x).permute(0, 2, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class RefinedDualBiS6TAL(nn.Module):\n",
        "    \"\"\"Improved Dual-path S6 Architecture with enhanced temporal modeling\"\"\"\n",
        "    def __init__(self, num_classes: int, dim: int = 128, recur_steps: int = 4):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(3, dim, kernel_size=(3,7,7), stride=(1,2,2), padding=(1,3,3)),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool3d(kernel_size=(1,3,3), stride=(1,2,2), padding=(0,1,1)),\n",
        "            nn.BatchNorm3d(dim)\n",
        "        )\n",
        "\n",
        "        # Temporal modeling with attention\n",
        "        self.temporal_blocks = nn.ModuleList([\n",
        "            RefinedFeatureAggregatedBiS6(dim)\n",
        "            for _ in range(recur_steps)\n",
        "        ])\n",
        "\n",
        "        # Multi-scale pyramid with attention\n",
        "        self.pyramid = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                RefinedFeatureAggregatedBiS6(dim),\n",
        "                nn.MaxPool1d(2, stride=2)\n",
        "            ) for _ in range(4)\n",
        "        ])\n",
        "\n",
        "        # Prediction heads with confidence modeling\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.Conv1d(dim * 5, dim, 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(dim, dim//2, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(dim//2, num_classes, 1)\n",
        "        )\n",
        "\n",
        "        self.reg_head = nn.Sequential(\n",
        "            nn.Conv1d(dim * 5, dim, 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(dim, dim//2, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(dim//2, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.conf_head = nn.Sequential(\n",
        "            nn.Conv1d(dim * 5, dim//2, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(dim//2, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict:\n",
        "        # Initial features\n",
        "        x = self.encoder(x)\n",
        "        x = x.flatten(3).mean(-1)\n",
        "\n",
        "        # Temporal modeling\n",
        "        for block in self.temporal_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Multi-scale pyramid\n",
        "        pyramid_features = [x]\n",
        "        curr_feat = x\n",
        "        for branch in self.pyramid:\n",
        "            curr_feat = branch(curr_feat)\n",
        "            pyramid_features.append(\n",
        "                nn.functional.interpolate(\n",
        "                    curr_feat,\n",
        "                    size=x.shape[-1],\n",
        "                    mode='linear'\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Merge pyramid features\n",
        "        merged = torch.cat(pyramid_features, dim=1)\n",
        "\n",
        "        return {\n",
        "            'cls_logits': self.cls_head(merged),\n",
        "            'reg_outputs': self.reg_head(merged),\n",
        "            'confidence': self.conf_head(merged)\n",
        "        }"
      ],
      "metadata": {
        "id": "OJ2BHgCUhp36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoProcessor:\n",
        "    \"\"\"Handles video loading and preprocessing\"\"\"\n",
        "    def __init__(self, frame_size: int = 128, clip_len: int = 8):\n",
        "        self.frame_size = frame_size\n",
        "        self.clip_len = clip_len\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((frame_size, frame_size)),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def load_video(self, video_path: str) -> Tuple[torch.Tensor, float]:\n",
        "      \"\"\"Load and preprocess video frames\"\"\"\n",
        "      vr = decord.VideoReader(video_path)\n",
        "      total_frames = len(vr)\n",
        "      fps = vr.get_avg_fps()\n",
        "\n",
        "      # Sample frames uniformly\n",
        "      indices = np.linspace(0, total_frames-1, num=self.clip_len, dtype=int)\n",
        "      frames = vr.get_batch(indices).asnumpy()\n",
        "      frames = torch.from_numpy(frames).float() / 255.0\n",
        "      frames = frames.permute(0, 3, 1, 2)  # (T, C, H, W)\n",
        "\n",
        "      # Apply transforms\n",
        "      frames = self.transform(frames)\n",
        "\n",
        "      # Permute to match model input shape: (C, T, H, W)\n",
        "      frames = frames.permute(1, 0, 2, 3)\n",
        "\n",
        "      return frames, fps\n",
        "\n",
        "class InferenceEngine:\n",
        "    \"\"\"Handles model loading and inference\"\"\"\n",
        "    def __init__(self, checkpoint_path: str, class_labels_path: str, device: str = 'cuda'):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Load class labels\n",
        "        with open(class_labels_path, 'r') as f:\n",
        "            self.class_to_idx = json.load(f)\n",
        "            self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = RefinedDualBiS6TAL(\n",
        "            num_classes=len(self.class_to_idx),\n",
        "            dim=64,\n",
        "            recur_steps=2\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Load checkpoint\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "    def predict(self, frames: torch.Tensor, confidence_threshold: float = 0.5) -> List[Dict]:\n",
        "      \"\"\"Run inference and post-process results\"\"\"\n",
        "      with torch.no_grad():\n",
        "          # Prepare input\n",
        "          frames = frames.unsqueeze(0)  # Add batch dimension: (1, C, T, H, W)\n",
        "          frames = frames.to(self.device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = self.model(frames)\n",
        "\n",
        "          # Post-process predictions\n",
        "          cls_scores = torch.sigmoid(outputs['cls_logits'][0])  # Remove batch dim\n",
        "          reg_outputs = outputs['reg_outputs'][0]\n",
        "          confidence = outputs['confidence'][0]\n",
        "\n",
        "          # Extract predictions above threshold\n",
        "          predictions = []\n",
        "          for t in range(cls_scores.shape[0]):\n",
        "              for c in range(cls_scores.shape[1]):\n",
        "                  if cls_scores[t, c] * confidence[t, 0] > confidence_threshold:\n",
        "                      # Convert regression outputs to temporal boundaries\n",
        "                      center_offset = reg_outputs[t, 0]\n",
        "                      length = reg_outputs[t, 1] * 100  # Scale factor from training\n",
        "\n",
        "                      start_time = max(0, t + center_offset - length/2)\n",
        "                      end_time = min(cls_scores.shape[0], t + center_offset + length/2)\n",
        "\n",
        "                      predictions.append({\n",
        "                          'action': self.idx_to_class[c],\n",
        "                          'start_frame': int(start_time),\n",
        "                          'end_frame': int(end_time),\n",
        "                          'confidence': float(cls_scores[t, c] * confidence[t, 0])\n",
        "                      })\n",
        "\n",
        "          return predictions\n",
        "\n",
        "class VideoAnnotator:\n",
        "    \"\"\"Handles video visualization and annotation\"\"\"\n",
        "    def __init__(self, output_path: str):\n",
        "        self.output_path = output_path\n",
        "\n",
        "    def annotate_video(self, video_path: str, predictions: List[Dict], fps: float):\n",
        "        \"\"\"Create annotated video with predictions\"\"\"\n",
        "        # Open input video\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "        # Create output video writer\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(\n",
        "            self.output_path,\n",
        "            fourcc,\n",
        "            fps,\n",
        "            (width, height)\n",
        "        )\n",
        "\n",
        "        frame_idx = 0\n",
        "        with tqdm(total=int(cap.get(cv2.CAP_PROP_FRAME_COUNT))) as pbar:\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Find active predictions for current frame\n",
        "                active_preds = [\n",
        "                    p for p in predictions\n",
        "                    if p['start_frame'] <= frame_idx <= p['end_frame']\n",
        "                ]\n",
        "\n",
        "                # Draw predictions\n",
        "                for pred in active_preds:\n",
        "                    # Add text overlay\n",
        "                    text = f\"{pred['action']} ({pred['confidence']:.2f})\"\n",
        "                    cv2.putText(\n",
        "                        frame,\n",
        "                        text,\n",
        "                        (50, 50),  # Position can be adjusted\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        1,\n",
        "                        (0, 255, 0),\n",
        "                        2\n",
        "                    )\n",
        "\n",
        "                out.write(frame)\n",
        "                frame_idx += 1\n",
        "                pbar.update(1)\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "def run_inference(\n",
        "    video_path: str,\n",
        "    checkpoint_path: str,\n",
        "    class_labels_path: str,\n",
        "    output_path: str,\n",
        "    confidence_threshold: float = 0.5\n",
        "):\n",
        "    \"\"\"Main inference pipeline\"\"\"\n",
        "    # Initialize components\n",
        "    processor = VideoProcessor()\n",
        "    engine = InferenceEngine(checkpoint_path, class_labels_path)\n",
        "    annotator = VideoAnnotator(output_path)\n",
        "\n",
        "    # Process video\n",
        "    frames, fps = processor.load_video(video_path)\n",
        "\n",
        "    # Run inference\n",
        "    predictions = engine.predict(frames, confidence_threshold)\n",
        "\n",
        "    # Create annotated video\n",
        "    annotator.annotate_video(video_path, predictions, fps)\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "G2p6odB-fgzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import decord\n",
        "import cv2\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Modify the VideoProcessor class in the inference code\n",
        "class VideoProcessor:\n",
        "    \"\"\"Handles video loading and preprocessing\"\"\"\n",
        "    def __init__(self, frame_size: int = 128, num_clips: int = 32, temporal_stride: int = 4):\n",
        "        self.frame_size = frame_size\n",
        "        self.num_clips = num_clips\n",
        "        self.temporal_stride = temporal_stride\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((frame_size, frame_size)),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def load_video(self, video_path: str) -> Tuple[torch.Tensor, float]:\n",
        "        \"\"\"Load and preprocess video frames to match training setup\"\"\"\n",
        "        vr = decord.VideoReader(video_path)\n",
        "        total_frames = len(vr)\n",
        "        fps = vr.get_avg_fps()\n",
        "\n",
        "        # Sample frames using the same parameters as during training\n",
        "        num_frames = self.num_clips * self.temporal_stride\n",
        "        indices = np.linspace(0, total_frames-1, num=num_frames, dtype=int)\n",
        "        frames = vr.get_batch(indices).asnumpy()\n",
        "        frames = torch.from_numpy(frames).float() / 255.0\n",
        "        frames = frames.permute(0, 3, 1, 2)  # (T, C, H, W)\n",
        "\n",
        "        # Apply transforms\n",
        "        frames = self.transform(frames)\n",
        "\n",
        "        # Permute to match model input shape: (C, T, H, W)\n",
        "        frames = frames.permute(1, 0, 2, 3)\n",
        "\n",
        "        return frames, fps\n",
        "\n",
        "class InferenceEngine:\n",
        "    \"\"\"Handles model loading and inference\"\"\"\n",
        "    def __init__(self, checkpoint_path: str, class_labels_path: str, device: str = 'cuda'):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Load class labels\n",
        "        with open(class_labels_path, 'r') as f:\n",
        "            self.class_to_idx = json.load(f)\n",
        "            self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n",
        "\n",
        "            print(self.idx_to_class)\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = RefinedDualBiS6TAL(\n",
        "            num_classes=len(self.class_to_idx),\n",
        "            dim=64,\n",
        "            recur_steps=2\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Load checkpoint\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "    def predict(self, frames: torch.Tensor, confidence_threshold: float = 0.5) -> List[Dict]:\n",
        "        \"\"\"Run inference and post-process results\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Prepare input\n",
        "            frames = frames.unsqueeze(0).to(self.device)  # Add batch dimension\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.model(frames)\n",
        "\n",
        "            # Post-process predictions\n",
        "            # Adjust dimensions: [C, T] -> [T, C]\n",
        "            cls_scores = torch.sigmoid(outputs['cls_logits'][0].permute(1, 0))  # (T, C)\n",
        "            reg_outputs = outputs['reg_outputs'][0].permute(1, 0)  # (T, 2)\n",
        "            confidence = outputs['confidence'][0].permute(1, 0)  # (T, 1)\n",
        "\n",
        "            # Extract predictions above threshold\n",
        "            predictions = []\n",
        "            for t in range(cls_scores.shape[0]):  # Iterate over temporal dimension\n",
        "                for c in range(cls_scores.shape[1]):  # Iterate over classes\n",
        "                    if cls_scores[t, c] * confidence[t, 0] > confidence_threshold:\n",
        "                        # Convert regression outputs to temporal boundaries\n",
        "                        center_offset = reg_outputs[t, 0]\n",
        "                        length = reg_outputs[t, 1] * 100  # Scale factor from training\n",
        "\n",
        "                        start_time = max(0, t + center_offset - length/2)\n",
        "                        end_time = min(cls_scores.shape[0], t + center_offset + length/2)\n",
        "\n",
        "\n",
        "                        print()\n",
        "\n",
        "                        predictions.append({\n",
        "                            'action': self.idx_to_class[int(c)],\n",
        "                            'start_frame': int(start_time),\n",
        "                            'end_frame': int(end_time),\n",
        "                            'confidence': float(cls_scores[t, c] * confidence[t, 0])\n",
        "                        })\n",
        "\n",
        "            return predictions\n",
        "\n",
        "class VideoAnnotator:\n",
        "    \"\"\"Handles video visualization and annotation\"\"\"\n",
        "    def __init__(self, output_path: str):\n",
        "        self.output_path = output_path\n",
        "\n",
        "    def annotate_video(self, video_path: str, predictions: List[Dict], fps: float):\n",
        "        \"\"\"Create annotated video with predictions\"\"\"\n",
        "        # Open input video\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "        # Create output video writer\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(\n",
        "            self.output_path,\n",
        "            fourcc,\n",
        "            fps,\n",
        "            (width, height)\n",
        "        )\n",
        "\n",
        "        frame_idx = 0\n",
        "        with tqdm(total=int(cap.get(cv2.CAP_PROP_FRAME_COUNT))) as pbar:\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Find active predictions for current frame\n",
        "                active_preds = [\n",
        "                    p for p in predictions\n",
        "                    if p['start_frame'] <= frame_idx <= p['end_frame']\n",
        "                ]\n",
        "\n",
        "                # Draw predictions\n",
        "                for pred in active_preds:\n",
        "                    # Add text overlay\n",
        "                    text = f\"{pred['action']} ({pred['confidence']:.2f})\"\n",
        "                    cv2.putText(\n",
        "                        frame,\n",
        "                        text,\n",
        "                        (50, 50),  # Position can be adjusted\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        1,\n",
        "                        (0, 255, 0),\n",
        "                        2\n",
        "                    )\n",
        "\n",
        "                out.write(frame)\n",
        "                frame_idx += 1\n",
        "                pbar.update(1)\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "\n",
        "def run_inference(\n",
        "    video_path: str,\n",
        "    checkpoint_path: str,\n",
        "    class_labels_path: str,\n",
        "    output_path: str,\n",
        "    confidence_threshold: float = 0.5\n",
        "):\n",
        "    \"\"\"Main inference pipeline\"\"\"\n",
        "    # Initialize components\n",
        "    processor = VideoProcessor()\n",
        "    engine = InferenceEngine(checkpoint_path, class_labels_path)\n",
        "    annotator = VideoAnnotator(output_path)\n",
        "\n",
        "    # Process video\n",
        "    frames, fps = processor.load_video(video_path)\n",
        "\n",
        "    # Run inference\n",
        "    predictions = engine.predict(frames, confidence_threshold)\n",
        "\n",
        "    # Create annotated video\n",
        "    annotator.annotate_video(video_path, predictions, fps)\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "qNxqiAgigLkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predictions = run_inference(\n",
        "    video_path=\"/content/WhatsApp Video 2025-02-16 at 18.33.40.mp4\",\n",
        "    checkpoint_path=\"/content/checkpoint_epoch_20.pth\",\n",
        "    class_labels_path=\"/content/class_labels.txt\",\n",
        "    output_path=\"annotated_video.mp4\",\n",
        "    confidence_threshold=0.1\n",
        ")\n",
        "\n",
        "# Print predictions\n",
        "for pred in predictions:\n",
        "    print(\n",
        "        f\"Action: {pred['action']}, \"\n",
        "        f\"Time: {pred['start_frame']}-{pred['end_frame']}, \"\n",
        "        f\"Confidence: {pred['confidence']:.2f}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL6qKbFhkqVY",
        "outputId": "0512918c-710e-4018-cf01-6329980d1fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'Archery', 1: 'Ballet', 2: 'Bathing dog', 3: 'Belly dance', 4: 'Brushing hair', 5: 'Brushing teeth', 6: 'Doing nails', 7: 'Playing guitarra', 8: 'Smoking a cigarette', 9: 'Spinning'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-277f5ef45155>:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=self.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6098/6098 [00:32<00:00, 186.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action: Bathing dog, Time: 1--1, Confidence: 0.10\n",
            "Action: Playing guitarra, Time: 1--1, Confidence: 0.13\n",
            "Action: Archery, Time: 124-128, Confidence: 0.16\n",
            "Action: Ballet, Time: 124-128, Confidence: 0.10\n",
            "Action: Belly dance, Time: 124-128, Confidence: 0.13\n",
            "Action: Spinning, Time: 124-128, Confidence: 0.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC6JTccEkyap",
        "outputId": "5fe8a2b1-4220-45f9-ca12-97acabab41a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from flask import Flask, Response\n",
        "from threading import Thread\n",
        "\n",
        "# Load your model\n",
        "model = torch.load(\"checkpoint_epoch_20.pth\", map_location=torch.device('cpu'))\n",
        "model.eval()\n",
        "\n",
        "def process_frame(frame):\n",
        "    \"\"\"Preprocess frame and perform inference.\"\"\"\n",
        "    frame = cv2.resize(frame, (224, 224))  # Resize to match model input\n",
        "    frame_tensor = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
        "    with torch.no_grad():\n",
        "        output = model(frame_tensor)\n",
        "    return output.cpu().numpy()\n",
        "\n",
        "# Flask app for video streaming\n",
        "app = Flask(__name__)\n",
        "capture = cv2.VideoCapture(0)  # Use default laptop camera\n",
        "external_camera = cv2.VideoCapture(1)  # Attempt to connect to an external camera\n",
        "\n",
        "def generate_frames(source):\n",
        "    while True:\n",
        "        success, frame = source.read()\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        # Process frame through the model\n",
        "        result = process_frame(frame)\n",
        "\n",
        "        # Overlay result on frame\n",
        "        cv2.putText(frame, f\"Prediction: {result}\", (10, 30),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        _, buffer = cv2.imencode('.jpg', frame)\n",
        "        frame_bytes = buffer.tobytes()\n",
        "        yield (b'--frame\\r\\n'\n",
        "               b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame_bytes + b'\\r\\n')\n",
        "\n",
        "@app.route('/video_feed/laptop')\n",
        "def video_feed_laptop():\n",
        "    return Response(generate_frames(capture), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
        "\n",
        "@app.route('/video_feed/external')\n",
        "def video_feed_external():\n",
        "    return Response(generate_frames(external_camera), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n"
      ],
      "metadata": {
        "id": "i8_nZltHAbbB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}